import pickle

save_path = './tests/materials/document_object.pkl'

with open(save_path, 'rb') as f:
    document_object = pickle.load(f)


print(document_object)

# ì´ì œ ë§Œë“¤ì–´ì•¼ í•˜ëŠ” ê²ƒì€ 
# Document AIì˜ document_objectì—ì„œ íŠ¹ì • **í…ìŠ¤íŠ¸ ë²”ìœ„(ì‹œì‘ ì¸ë±ìŠ¤ ~ ë ì¸ë±ìŠ¤)**ì— í¬í•¨ëœ **í† í°(token)**ì„ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜

def find_tokens_in_range(document_object, start_index, end_index):
    """
    íŠ¹ì • ì‹œì‘~ë ì¸ë±ìŠ¤ì— í¬í•¨ë˜ëŠ” ëª¨ë“  í† í°ì„ ê²€ìƒ‰í•˜ëŠ” í•¨ìˆ˜.

    Args:
        document_object: Google Document AIì˜ OCR ê²°ê³¼ ê°ì²´
        start_index (int): ê²€ìƒ‰í•  ì‹œì‘ ì¸ë±ìŠ¤
        end_index (int): ê²€ìƒ‰í•  ë ì¸ë±ìŠ¤

    Returns:
        list: ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í† í° ëª©ë¡ (ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ë°˜í™˜)
    """
    matched_tokens = []

    for page in document_object.pages:
        for token in page.tokens:
            # í† í°ì´ text_anchor.text_segmentsë¥¼ ê°€ì§€ê³  ìˆëŠ”ì§€ í™•ì¸
            if not token.layout.text_anchor.text_segments:
                continue  # text_segmentsê°€ ì—†ëŠ” ê²½ìš° ê±´ë„ˆëœ€

            # í˜„ì¬ í† í°ì˜ ì‹œì‘/ë ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
            token_start = token.layout.text_anchor.text_segments[0].start_index
            token_end = token.layout.text_anchor.text_segments[0].end_index

            # âœ… í† í°ì´ ë²”ìœ„ ë‚´ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
            if token_start < end_index and token_end > start_index:
                matched_tokens.append({
                    "text": document_object.text[token_start:token_end],  # ì‹¤ì œ í…ìŠ¤íŠ¸
                    "start_index": token_start,
                    "end_index": token_end,
                    "page_number": page.page_number,
                    "normalized_vertices": [
                        {"x": vertex.x, "y": vertex.y} for vertex in token.layout.bounding_poly.normalized_vertices
                    ]  # ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸ ë³€í™˜
                })

    return matched_tokens



# ğŸ”¥ ì˜ˆì œ ì‹¤í–‰
start_index = 6
end_index = 14
tokens_in_range = find_tokens_in_range(document_object, start_index, end_index)

# ê²°ê³¼ ì¶œë ¥
for token in tokens_in_range:
    print(f"í† í°: {token['text']}, ì‹œì‘: {token['start_index']}, ë: {token['end_index']}, í˜ì´ì§€: {token['page_number']}")
